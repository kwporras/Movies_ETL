{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "993446c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "from config import db_password\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c15eb5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directory\n",
    "file_dir = 'C:\\\\Users\\keyto\\git\\Modules\\Module_8\\Movies_ETL\\Resources'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba10763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f'{file_dir}filename'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb6fb28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file into variable\n",
    "with open(f'{file_dir}\\wikipedia-movies.json', mode='r') as file:\n",
    "    wiki_movies_raw = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35593c21",
   "metadata": {},
   "source": [
    "### Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f634a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7311"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check lengh of file\n",
    "len(wiki_movies_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6f1abc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://en.wikipedia.org/wiki/The_Adventures_of_Ford_Fairlane',\n",
       "  'year': 1990,\n",
       "  'imdb_link': 'https://www.imdb.com/title/tt0098987/',\n",
       "  'title': 'The Adventures of Ford Fairlane',\n",
       "  'Directed by': 'Renny Harlin',\n",
       "  'Produced by': ['Steve Perry', 'Joel Silver'],\n",
       "  'Screenplay by': ['David Arnott', 'James Cappe', 'Daniel Waters'],\n",
       "  'Story by': ['David Arnott', 'James Cappe'],\n",
       "  'Based on': ['Characters', 'by Rex Weiner'],\n",
       "  'Starring': ['Andrew Dice Clay',\n",
       "   'Wayne Newton',\n",
       "   'Priscilla Presley',\n",
       "   'Lauren Holly',\n",
       "   'Morris Day',\n",
       "   'Robert Englund',\n",
       "   \"Ed O'Neill\"],\n",
       "  'Narrated by': 'Andrew \"Dice\" Clay',\n",
       "  'Music by': ['Cliff Eidelman', 'Yello'],\n",
       "  'Cinematography': 'Oliver Wood',\n",
       "  'Edited by': 'Michael Tronick',\n",
       "  'Productioncompany ': 'Silver Pictures',\n",
       "  'Distributed by': '20th Century Fox',\n",
       "  'Release date': ['July 11, 1990', '(', '1990-07-11', ')'],\n",
       "  'Running time': '102 minutes',\n",
       "  'Country': 'United States',\n",
       "  'Language': 'English',\n",
       "  'Budget': '$20 million',\n",
       "  'Box office': '$21.4 million'},\n",
       " {'url': 'https://en.wikipedia.org/wiki/After_Dark,_My_Sweet',\n",
       "  'year': 1990,\n",
       "  'imdb_link': 'https://www.imdb.com/title/tt0098994/',\n",
       "  'title': 'After Dark, My Sweet',\n",
       "  'Directed by': 'James Foley',\n",
       "  'Produced by': ['Ric Kidney', 'Robert Redlin'],\n",
       "  'Screenplay by': ['James Foley', 'Robert Redlin'],\n",
       "  'Based on': ['the novel', 'After Dark, My Sweet', 'by', 'Jim Thompson'],\n",
       "  'Starring': ['Jason Patric',\n",
       "   'Rachel Ward',\n",
       "   'Bruce Dern',\n",
       "   'George Dickerson'],\n",
       "  'Music by': 'Maurice Jarre',\n",
       "  'Cinematography': 'Mark Plummer',\n",
       "  'Edited by': 'Howard E. Smith',\n",
       "  'Productioncompany ': 'Avenue Pictures',\n",
       "  'Distributed by': 'Avenue Pictures',\n",
       "  'Release date': ['May 17, 1990',\n",
       "   '(',\n",
       "   '1990-05-17',\n",
       "   ')',\n",
       "   '(Cannes Film Market)',\n",
       "   'August 24, 1990',\n",
       "   '(',\n",
       "   '1990-08-24',\n",
       "   ')',\n",
       "   '(United States)'],\n",
       "  'Running time': '114 minutes',\n",
       "  'Country': 'United States',\n",
       "  'Language': 'English',\n",
       "  'Budget': '$6 million',\n",
       "  'Box office': '$2.7 million'},\n",
       " {'url': 'https://en.wikipedia.org/wiki/Air_America_(film)',\n",
       "  'year': 1990,\n",
       "  'imdb_link': 'https://www.imdb.com/title/tt0099005/',\n",
       "  'title': 'Air America',\n",
       "  'Directed by': 'Roger Spottiswoode',\n",
       "  'Produced by': 'Daniel Melnick',\n",
       "  'Screenplay by': ['John Eskow', 'Richard Rush'],\n",
       "  'Based on': ['Air America', 'by', 'Christopher Robbins'],\n",
       "  'Starring': ['Mel Gibson',\n",
       "   'Robert Downey Jr.',\n",
       "   'Nancy Travis',\n",
       "   'David Marshall Grant',\n",
       "   'Lane Smith'],\n",
       "  'Music by': 'Charles Gross',\n",
       "  'Cinematography': 'Roger Deakins',\n",
       "  'Edited by': ['John Bloom', 'Lois Freeman-Fox'],\n",
       "  'Productioncompany ': ['Carolco Pictures', 'IndieProd Company'],\n",
       "  'Distributed by': 'TriStar Pictures',\n",
       "  'Release date': ['August 10, 1990', '(', '1990-08-10', ')'],\n",
       "  'Running time': '113 minutes',\n",
       "  'Country': 'United States',\n",
       "  'Language': ['English', 'Lao'],\n",
       "  'Budget': '$35 million',\n",
       "  'Box office': '$57,718,089'},\n",
       " {'url': 'https://en.wikipedia.org/wiki/Alice_(1990_film)',\n",
       "  'year': 1990,\n",
       "  'imdb_link': 'https://www.imdb.com/title/tt0099012/',\n",
       "  'title': 'Alice',\n",
       "  'Directed by': 'Woody Allen',\n",
       "  'Produced by': 'Robert Greenhut',\n",
       "  'Written by': 'Woody Allen',\n",
       "  'Starring': ['Alec Baldwin',\n",
       "   'Blythe Danner',\n",
       "   'Judy Davis',\n",
       "   'Mia Farrow',\n",
       "   'William Hurt',\n",
       "   'Keye Luke',\n",
       "   'Joe Mantegna',\n",
       "   'Bernadette Peters'],\n",
       "  'Cinematography': 'Carlo Di Palma',\n",
       "  'Edited by': 'Susan E. Morse',\n",
       "  'Distributed by': 'Orion Pictures',\n",
       "  'Release date': ['December 25, 1990', '(', '1990-12-25', ')'],\n",
       "  'Running time': '106 minutes',\n",
       "  'Country': 'United States',\n",
       "  'Language': 'English',\n",
       "  'Budget': '$12 million',\n",
       "  'Box office': '$7,331,647'},\n",
       " {'url': 'https://en.wikipedia.org/wiki/Almost_an_Angel',\n",
       "  'year': 1990,\n",
       "  'imdb_link': 'https://www.imdb.com/title/tt0099018/',\n",
       "  'title': 'Almost an Angel',\n",
       "  'Directed by': 'John Cornell',\n",
       "  'Produced by': 'John Cornell',\n",
       "  'Written by': 'Paul Hogan',\n",
       "  'Starring': ['Paul Hogan', 'Elias Koteas', 'Linda Kozlowski'],\n",
       "  'Music by': 'Maurice Jarre',\n",
       "  'Cinematography': 'Russell Boyd',\n",
       "  'Edited by': 'David Stiven',\n",
       "  'Distributed by': 'Paramount Pictures',\n",
       "  'Release date': 'December 19, 1990',\n",
       "  'Running time': '95 minutes',\n",
       "  'Country': 'US',\n",
       "  'Language': 'English',\n",
       "  'Budget': '$25 million',\n",
       "  'Box office': '$6,939,946 (USA)'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 5 records - index slice\n",
    "wiki_movies_raw[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "259d7335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://en.wikipedia.org/wiki/Holmes_%26_Watson',\n",
       "  'year': 2018,\n",
       "  'imdb_link': 'https://www.imdb.com/title/tt1255919/',\n",
       "  'title': 'Holmes & Watson',\n",
       "  'Directed by': 'Etan Cohen',\n",
       "  'Produced by': ['Will Ferrell',\n",
       "   'Adam McKay',\n",
       "   'Jimmy Miller',\n",
       "   'Clayton Townsend'],\n",
       "  'Screenplay by': 'Etan Cohen',\n",
       "  'Based on': ['Sherlock Holmes',\n",
       "   'and',\n",
       "   'Dr. Watson',\n",
       "   'by',\n",
       "   'Sir Arthur Conan Doyle'],\n",
       "  'Starring': ['Will Ferrell',\n",
       "   'John C. Reilly',\n",
       "   'Rebecca Hall',\n",
       "   'Rob Brydon',\n",
       "   'Steve Coogan',\n",
       "   'Ralph Fiennes'],\n",
       "  'Music by': 'Mark Mothersbaugh',\n",
       "  'Cinematography': 'Oliver Wood',\n",
       "  'Edited by': 'Dean Zimmerman',\n",
       "  'Productioncompanies ': ['Columbia Pictures',\n",
       "   'Gary Sanchez Productions',\n",
       "   'Mosaic Media Group',\n",
       "   'Mimran Schur Pictures'],\n",
       "  'Distributed by': 'Sony Pictures Releasing',\n",
       "  'Release date': ['December 25, 2018',\n",
       "   '(',\n",
       "   '2018-12-25',\n",
       "   ')',\n",
       "   '(United States)'],\n",
       "  'Running time': '90 minutes',\n",
       "  'Country': 'United States',\n",
       "  'Language': 'English',\n",
       "  'Budget': '$42 million',\n",
       "  'Box office': '$41.9 million'},\n",
       " {'url': 'https://en.wikipedia.org/wiki/Vice_(2018_film)',\n",
       "  'year': 2018,\n",
       "  'imdb_link': 'https://www.imdb.com/title/tt6266538/',\n",
       "  'title': 'Vice',\n",
       "  'Directed by': 'Adam McKay',\n",
       "  'Produced by': ['Brad Pitt',\n",
       "   'Dede Gardner',\n",
       "   'Jeremy Kleiner',\n",
       "   'Kevin J. Messick',\n",
       "   'Will Ferrell',\n",
       "   'Adam McKay'],\n",
       "  'Written by': 'Adam McKay',\n",
       "  'Starring': ['Christian Bale',\n",
       "   'Amy Adams',\n",
       "   'Steve Carell',\n",
       "   'Sam Rockwell',\n",
       "   'Tyler Perry',\n",
       "   'Alison Pill',\n",
       "   'Lily Rabe',\n",
       "   'Jesse Plemons'],\n",
       "  'Music by': 'Nicholas Britell',\n",
       "  'Cinematography': 'Greig Fraser',\n",
       "  'Edited by': 'Hank Corwin',\n",
       "  'Productioncompany ': ['Plan B Entertainment',\n",
       "   'Gary Sanchez Productions',\n",
       "   'Annapurna Pictures'],\n",
       "  'Distributed by': 'Mirror Releasing',\n",
       "  'Release date': ['December 11, 2018',\n",
       "   '(',\n",
       "   '2018-12-11',\n",
       "   ')',\n",
       "   '(',\n",
       "   'Samuel Goldwyn Theater',\n",
       "   ')',\n",
       "   'December 25, 2018',\n",
       "   '(',\n",
       "   '2018-12-25',\n",
       "   ')',\n",
       "   '(United States)'],\n",
       "  'Running time': '132 minutes',\n",
       "  'Country': 'United States',\n",
       "  'Language': 'English',\n",
       "  'Budget': '$60 million',\n",
       "  'Box office': '$76.1 million'},\n",
       " {'url': 'https://en.wikipedia.org/wiki/On_the_Basis_of_Sex',\n",
       "  'year': 2018,\n",
       "  'imdb_link': 'https://www.imdb.com/title/tt4669788/',\n",
       "  'title': 'On the Basis of Sex',\n",
       "  'Directed by': 'Mimi Leder',\n",
       "  'Produced by': 'Robert W. Cort',\n",
       "  'Written by': 'Daniel Stiepleman',\n",
       "  'Starring': ['Felicity Jones',\n",
       "   'Armie Hammer',\n",
       "   'Justin Theroux',\n",
       "   'Sam Waterston',\n",
       "   'Kathy Bates'],\n",
       "  'Music by': 'Mychael Danna',\n",
       "  'Cinematography': 'Michael Grady',\n",
       "  'Edited by': 'Michelle Tesoro',\n",
       "  'Productioncompanies ': ['Focus Features',\n",
       "   '[1]',\n",
       "   'Participant Media',\n",
       "   '[1]',\n",
       "   'Robert Cort Productions',\n",
       "   '[1]',\n",
       "   'Alibaba Pictures',\n",
       "   '[2]'],\n",
       "  'Distributed by': 'Focus Features',\n",
       "  'Release date': ['November 8, 2018',\n",
       "   '(',\n",
       "   '2018-11-08',\n",
       "   ')',\n",
       "   '(',\n",
       "   'AFI Fest',\n",
       "   ')',\n",
       "   'December 25, 2018',\n",
       "   '(',\n",
       "   '2018-12-25',\n",
       "   ')',\n",
       "   '(United States)'],\n",
       "  'Running time': '120 minutes',\n",
       "  'Country': 'United States',\n",
       "  'Language': 'English',\n",
       "  'Budget': '$20 million',\n",
       "  'Box office': '$38.4 million'},\n",
       " {'url': 'https://en.wikipedia.org/wiki/Destroyer_(2018_film)',\n",
       "  'year': 2018,\n",
       "  'imdb_link': 'https://www.imdb.com/title/tt7137380/',\n",
       "  'title': 'Destroyer',\n",
       "  'Directed by': 'Karyn Kusama',\n",
       "  'Produced by': ['Fred Berger', 'Phil Hay', 'Matt Manfredi'],\n",
       "  'Written by': ['Phil Hay', 'Matt Manfredi'],\n",
       "  'Starring': ['Nicole Kidman',\n",
       "   'Sebastian Stan',\n",
       "   'Toby Kebbell',\n",
       "   'Tatiana Maslany',\n",
       "   'Bradley Whitford',\n",
       "   'Jade Pettyjohn',\n",
       "   'Scoot McNairy'],\n",
       "  'Music by': 'Theodore Shapiro',\n",
       "  'Cinematography': 'Julie Kirkwood',\n",
       "  'Edited by': 'Plummy Tucker',\n",
       "  'Productioncompany ': ['30West',\n",
       "   'Automatik Entertainment',\n",
       "   'Annapurna Pictures'],\n",
       "  'Distributed by': 'Mirror Releasing',\n",
       "  'Release date': ['August 31, 2018',\n",
       "   '(',\n",
       "   '2018-08-31',\n",
       "   ')',\n",
       "   '(',\n",
       "   'Telluride',\n",
       "   ')',\n",
       "   'December 25, 2018',\n",
       "   '(',\n",
       "   '2018-12-25',\n",
       "   ')',\n",
       "   '(United States)'],\n",
       "  'Running time': '123 minutes',\n",
       "  'Country': 'United States',\n",
       "  'Language': 'English',\n",
       "  'Budget': '$9 million',\n",
       "  'Box office': '$5.5 million'},\n",
       " {'url': 'https://en.wikipedia.org/wiki/Black_Mirror:_Bandersnatch',\n",
       "  'year': 2018,\n",
       "  'imdb_link': 'https://www.imdb.com/title/tt9495224/',\n",
       "  'title': 'Bandersnatch',\n",
       "  'Directed by': 'David Slade',\n",
       "  'Produced by': 'Russell McLean',\n",
       "  'Written by': 'Charlie Brooker',\n",
       "  'Starring': ['Fionn Whitehead',\n",
       "   'Will Poulter',\n",
       "   'Craig Parkinson',\n",
       "   'Alice Lowe',\n",
       "   'Asim Chaudhry'],\n",
       "  'Music by': 'Brian Reitzell',\n",
       "  'Cinematography': ['Aaron Morton', 'Jake Polonsky'],\n",
       "  'Edited by': 'Tony Kearns',\n",
       "  'Productioncompany ': ['House of Tomorrow', 'Netflix'],\n",
       "  'Distributed by': 'Netflix',\n",
       "  'Release date': ['28 December 2018', '(', '2018-12-28', ')'],\n",
       "  'Running time': 'Variable; 90 minutes for default path',\n",
       "  'Country': 'United Kingdom',\n",
       "  'Language': 'English'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last 5 records - Negative index slice\n",
    "wiki_movies_raw[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ac8b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some records in the middle\n",
    "wiki_movies_raw[3600:3605]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bceb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_metadata = pd.read_csv(f'{file_dir}\\\\movies_metadata.csv', low_memory=False)\n",
    "ratings = pd.read_csv(f'{file_dir}\\\\ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2e53c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8557660",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d4b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_metadata.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f26e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31c4583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Inspect]\n",
    "\n",
    "# --- Begin Initial Investigation --- \n",
    "wiki_movies_df = pd.DataFrame(wiki_movies_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b24fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review top\n",
    "wiki_movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580e2bf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Review list of column names\n",
    "wiki_movies_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eaeece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# We can identify column names that don't relate to movie data,\n",
    "# such as \"Dewey Decimal,\" \"Headquarters,\" and \"Number of employees.\"\n",
    "# (There may be other examples that jumped out at you as well.)\n",
    "# Let's modify our JSON data by restricting it to only those entries\n",
    "# that have a director and an IMDb link.\n",
    "# We can do this with a list comprehension.\n",
    "\n",
    "# We've used list comprehensions previously as a compact way to apply\n",
    "# a function to every element in a list. \n",
    "\n",
    "# So far, we've used list comprehensions in the form to compress code\n",
    "# that would have been done in a for loop.\n",
    "\n",
    "# [expression for element in source_list]\n",
    "\n",
    "# We can also filter out results using a conditional filter expression,\n",
    "# as shown below:\n",
    "\n",
    "# [expression for element in source_list if filter_expression]\n",
    "\n",
    "# The resulting list will only have elements where the filter expression\n",
    "# evaluates to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3558f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# To create a filter expression for only movies with a director and an \n",
    "# IMDb link, keep in mind that there are two columns in the data for \n",
    "# director information. We'll need to check if either \"Director\" or\n",
    "# \"Directed by\" are keys in the current dict. If there is a director listed,\n",
    "# we also want to check that the dict has an IMDb link.\n",
    "# Luckily, that information is only in one column, imdb_link,\n",
    "# so our filter expression will look like the following:\n",
    "\n",
    "# if ('Director' in movie or 'Directed by' in movie) and 'imdb_link' in movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e2af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Execute]\n",
    "\n",
    "# Create a list comprehension with the filter expression we created\n",
    "# and save that to an intermediate variable wiki_movies.\n",
    "# See how many movies are in wiki_movies with the len() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc4337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use List Comprehenshion to Filter Data\n",
    "wiki_movies = [movie for movie in wiki_movies_raw\n",
    "               if ('Director' in movie or 'Directed by' in movie)\n",
    "                   and 'imdb_link' in movie]\n",
    "len(wiki_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe63688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Inspect]\n",
    "\n",
    "# 78 columns are still a lot of columns, so let's keep investigating.\n",
    "wiki_movies_df = pd.DataFrame(wiki_movies)\n",
    "wiki_movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56b9425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Inspect]\n",
    "# There sure are a lot of languages—we'll get to those shortly. \n",
    "# For now, one of the columns that stands out is \"No. of episodes.\"\n",
    "\n",
    "# [Plan]\n",
    "# It looks like we've got some TV shows in our data instead of movies.\n",
    "# We'll want to get rid of those, too.\n",
    "\n",
    "#[Execute]\n",
    "# We'll add that filter to our list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efe2244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Execute]\n",
    "\n",
    "# Add filter to our list comprehension.\n",
    "wiki_movies = [movie for movie in wiki_movies_raw\n",
    "               if ('Director' in movie or 'Directed by' in movie)\n",
    "                   and 'imdb_link' in movie\n",
    "                   and 'No. of episodes' not in movie]\n",
    "\n",
    "len(wiki_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e6ec24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wiki_movies_df = pd.DataFrame(wiki_movies)\n",
    "# wiki_movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_movies\n",
    "# THere are four basic parts to a function:\n",
    "# 1. Name\n",
    "# 2. Parameters\n",
    "# 3. Code block\n",
    "# 4. Return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e958dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda Functions\n",
    "# There's a special function we can make in Python called lambda,\n",
    "# which is the most stripped-down kind we can make. Example below:\n",
    "\n",
    "# lambda arguments: expression\n",
    "\n",
    "# This function will take in an argument and will return the expression.\n",
    "# Even though lambda functions are supposed to be anonymous,\n",
    "# just this once we'll create a lambda function and assign a name so that\n",
    "# we can see how they work.\n",
    "\n",
    "# A lambda function that squares a value looks like the following:\n",
    "# lambda x: x * x\n",
    "\n",
    "# Here, x is the argument, and x * x is the expression.\n",
    "# Let's assign this to a name so that we can use it:\n",
    "# square = lambda x: x * x\n",
    "# square(5)\n",
    "\n",
    "# The output will be\n",
    "\n",
    "# 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cfa6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the function clean_movie and have it take movie as a parameter\n",
    "# def clean_movie(movie):\n",
    "    \n",
    "    # Because the movies are dicts and we want to make nondestructive edits, make a copy of the incoming movie.\n",
    "    # To make a copy of movie, we'll use the dict() constructor.\n",
    "    \n",
    "    # Constructors are special functions that initialize new objects.\n",
    "    # They reserve space in memory for the object and perform any initializations the object requires.\n",
    "    # Also, constructors can take parameters and initialize a new object using those parameters.\n",
    "    # movie_copy = dict(movie)\n",
    "    \n",
    "    # However, we have another trick that's even better.\n",
    "    # Inside of the function, we can create a new local variable called movie and assign it the new copy of the parameter movie.\n",
    "    \n",
    "    # movie = dict(movie) #create a non-destructive copy\n",
    "    \n",
    "    #To finish our skeleton of the clean_movie function, return the movie variable.\n",
    "    # return movie\n",
    "\n",
    "# This function doesn't do much right now, but we'll be adding more to it soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9479887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Inspect]\n",
    "\n",
    "# Now take a look at what's going on with those languages.\n",
    "# The first one on the list is Arabic, so let's see which movies have a value for \"Arabic.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bc662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_movies_df[wiki_movies_df['Arabic'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190158ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_movies_df[wiki_movies_df['Arabic'].notnull()]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b6de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# The different language columns are for alternate titles of the movie.\n",
    "# Let's combine all of them into one dictionary that has all the alternate titles.\n",
    "\n",
    "# To do that, we need to go through each of the columns, one by one,\n",
    "# and determine which are alternate titles. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08d55f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(wiki_movies_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52baeaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ Plan]\n",
    "\n",
    "# Handle the Alternative Titles\n",
    "# Now we can add in code to handle the alternative titles.\n",
    "# The logic we need to implement follows:\n",
    "# 1. Make an empty dict to hold all of the alternative titles.\n",
    "# 2. Loop through a list of all alternative title keys:\n",
    "    # Check if the current key exists in the movie object.\n",
    "    # If so, remove the key-value pair and add to the alternative titles dict.\n",
    "# 3. After looping through every key, add the alternative titles dict to the movie object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b719c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Execute]\n",
    "\n",
    "#Call the function clean_movie and have it take movie as a parameter\n",
    "def clean_movie(movie):\n",
    "    \n",
    "    #create a non-destructive copy\n",
    "    movie = dict(movie) \n",
    "    \n",
    "    # [STEP 1] make empty dict to hold all of the alternative titles.\n",
    "    alt_titles = {} \n",
    "    \n",
    "    # [Step 2] Loop through a list of all alternative title keys\n",
    "    for key in ['Also known as','Arabic','Cantonese','Chinese','French',\n",
    "                'Hangul','Hebrew','Hepburn','Japanese','Literally',\n",
    "                'Mandarin','McCune–Reischauer','Original title','Polish',\n",
    "                'Revised Romanization','Romanized','Russian',\n",
    "                'Simplified','Traditional','Yiddish']:\n",
    "        \n",
    "        # [Step 2a] Check if the current key exists in the movie object.\n",
    "        if key in movie:\n",
    "    \n",
    "    # [Step 2a]\n",
    "    # return movie\n",
    "    \n",
    "            # [Step 2b] If so, remove the key-value pair and add to the alternative titles dictionary.\n",
    "            alt_titles[key] = movie[key]\n",
    "            movie.pop(key)  \n",
    "            \n",
    "    # [Step 3] After looping through every key, add the alternative titles dict to the movie object.\n",
    "    if len(alt_titles) > 0:\n",
    "        movie['alt_titles'] = alt_titles\n",
    "            \n",
    "    #To finish our skeleton of the clean_movie function, return the movie variable.\n",
    "    return movie\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d280e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of cleaned movies with a list comprehension:\n",
    "clean_movies = [clean_movie(movie) for movie in wiki_movies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876f4bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set wiki_movies_df to be the DataFrame created from clean_movies, and print out a list of columns.\n",
    "wiki_movies_df = pd.DataFrame(clean_movies)\n",
    "sorted(wiki_movies_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Inspect]\n",
    "\n",
    "# There are quite a few columns with slightly different names but the same data,\n",
    "# such as \"Directed by\" and \"Director.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caf5ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# We need to consolidate columns with the same data into one column.\n",
    "# We can use the pop() method to change the name of a dictionary key,\n",
    "# because pop() returns the value from the removed key-value pair.\n",
    "# We have to check if the key exists in a given movie record,\n",
    "# so it will be helpful to make a small function inside clean_movie().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3360a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Execute]\n",
    "\n",
    "# Our new function should look like the following.\n",
    "# Remember that this new function is enclosed within the\n",
    "# clean_movie function that we created earlier:\n",
    "\n",
    "    # def change_column_name(old_name, new_name):\n",
    "        # if old_name in movie:\n",
    "            # movie[new_name] = movie.pop(old_name)\n",
    "            \n",
    "    # To change every instance where the key is \"Directed by\" to the new key \"Director,\"\n",
    "    # write the following inside clean_movie():\n",
    "    \n",
    "    # change_column_name('Directed by', 'Director')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# There's no easy way around the next step:\n",
    "# we have to go through each column name and decide if there's a better name for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36beb474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Execute]\n",
    "\n",
    "    # change_column_name('Adaptation by', 'Writer(s)')\n",
    "    # change_column_name('Country of origin', 'Country')\n",
    "    # change_column_name('Directed by', 'Director')\n",
    "    # change_column_name('Distributed by', 'Distributor')\n",
    "    # change_column_name('Edited by', 'Editor(s)')\n",
    "    # change_column_name('Length', 'Running time')\n",
    "    # change_column_name('Original release', 'Release date')\n",
    "    # change_column_name('Music by', 'Composer(s)')\n",
    "    # change_column_name('Produced by', 'Producer(s)')\n",
    "    # change_column_name('Producer', 'Producer(s)')\n",
    "    # change_column_name('Productioncompanies ', 'Production company(s)')\n",
    "    # change_column_name('Productioncompany ', 'Production company(s)')\n",
    "    # change_column_name('Released', 'Release Date')\n",
    "    # change_column_name('Release Date', 'Release date')\n",
    "    # change_column_name('Screen story by', 'Writer(s)')\n",
    "    # change_column_name('Screenplay by', 'Writer(s)')\n",
    "    # change_column_name('Story by', 'Writer(s)')\n",
    "    # change_column_name('Theme music composer', 'Composer(s)')\n",
    "    # change_column_name('Written by', 'Writer(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df7dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function clean_movie() is starting to look a little complicated,\n",
    "# so we should add some commenting to make it easier to understand.\n",
    "# The whole function should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5225745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the function clean_movie and have it take movie as a parameter\n",
    "def clean_movie(movie):\n",
    "    \n",
    "    #create a non-destructive copy\n",
    "    movie = dict(movie) \n",
    "    \n",
    "    # make empty dict to hold all of the alternative titles.\n",
    "    alt_titles = {} \n",
    "    \n",
    "    # Loop through a list of all alternative title keys\n",
    "    for key in ['Also known as','Arabic','Cantonese','Chinese','French',\n",
    "                'Hangul','Hebrew','Hepburn','Japanese','Literally',\n",
    "                'Mandarin','McCune–Reischauer','Original title','Polish',\n",
    "                'Revised Romanization','Romanized','Russian',\n",
    "                'Simplified','Traditional','Yiddish']:\n",
    "        \n",
    "        # Check if the current key exists in the movie object.\n",
    "        if key in movie:\n",
    "            \n",
    "            # If so, remove the key-value pair and add to the alternative titles dictionary.\n",
    "            alt_titles[key] = movie[key]\n",
    "            movie.pop(key)  \n",
    "            \n",
    "    # After looping through every key, add the alternative titles dict to the movie object.\n",
    "    if len(alt_titles) > 0:\n",
    "        movie['alt_titles'] = alt_titles\n",
    "    \n",
    "    # merge column names\n",
    "    def change_column_name(old_name, new_name):\n",
    "        if old_name in movie:\n",
    "            movie[new_name] = movie.pop(old_name)\n",
    "    change_column_name('Adaptation by', 'Writer(s)')\n",
    "    change_column_name('Country of origin', 'Country')\n",
    "    change_column_name('Directed by', 'Director')\n",
    "    change_column_name('Distributed by', 'Distributor')\n",
    "    change_column_name('Edited by', 'Editor(s)')\n",
    "    change_column_name('Length', 'Running time')\n",
    "    change_column_name('Original release', 'Release date')\n",
    "    change_column_name('Music by', 'Composer(s)')\n",
    "    change_column_name('Produced by', 'Producer(s)')\n",
    "    change_column_name('Producer', 'Producer(s)')\n",
    "    change_column_name('Productioncompanies ', 'Production company(s)')\n",
    "    change_column_name('Productioncompany ', 'Production company(s)')\n",
    "    change_column_name('Released', 'Release Date')\n",
    "    change_column_name('Release Date', 'Release date')\n",
    "    change_column_name('Screen story by', 'Writer(s)')\n",
    "    change_column_name('Screenplay by', 'Writer(s)')\n",
    "    change_column_name('Story by', 'Writer(s)')\n",
    "    change_column_name('Theme music composer', 'Composer(s)')\n",
    "    change_column_name('Written by', 'Writer(s)')\n",
    "               \n",
    "    #To finish our skeleton of the clean_movie function, return the movie variable.\n",
    "    return movie\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31da52ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can rerun our list comprehension to clean wiki_movies and recreate wiki_movies_df.\n",
    "clean_movies = [clean_movie(movie) for movie in wiki_movies]\n",
    "wiki_movies_df = pd.DataFrame(clean_movies)\n",
    "sorted(wiki_movies_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48655bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# There are some data-cleaning tasks that are easier to perform on a DataFrame,\n",
    "# such as removing duplicate rows.\n",
    "# Luckily, we just created a process to turn our JSON data into a reasonable DataFrame.\n",
    "# In fact, we'll start by removing duplicate rows.\n",
    "\n",
    "# Since we're going to be using the IMDb ID to merge with the Kaggle data,\n",
    "# we want to make sure that we don't have any duplicate rows,\n",
    "# according to the IMDb ID.\n",
    "# First, we need to extract the IMDb ID from the IMDb link.\n",
    "\n",
    "# To extract the ID, we need to learn regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f68d87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expressions, also known as regex,\n",
    "# are strings of characters that define a search pattern.\n",
    "\n",
    "# First, we'll use regular expressions in Pandas' built-in string methods\n",
    "# that work on a Series object accessed with the str property.\n",
    "# We'll be using str.extract(), which takes in a regular expression pattern.\n",
    "# IMDb links generally look like \"https://www.imdb.com/title/tt1234567/,\"\n",
    "# with \"tt1234567\" as the IMDb ID. The regular expression for a group of characters that start with\n",
    "# \"tt\" and has seven digits is \"(tt\\d{7})\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3fa2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"(tt\\d{7})\" — The parentheses marks say to look for one group of text.\n",
    "# \"(tt\\d{7})\" — The \"tt\" in the string simply says to match two lowercase Ts.\n",
    "# \"(tt\\d{7})\" — The \"\\d\" says to match a numerical digit.\n",
    "# \"(tt\\d{7})\" — The \"{7}\" says to match the last thing (numerical digits) exactly seven times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1d833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since regular expressions use backslashes, which Python also uses for special characters,\n",
    "# we want to tell Python to treat our regular expression characters as a raw string of text.\n",
    "# Therefore, we put an r before the quotes. We need to do this every time we create a regular expression string.\n",
    "# We’ll put the extracted IMDB ID into a new column.\n",
    "# Altogether, the code to extract the IMDb ID looks like the following:\n",
    "\n",
    "# wiki_movies_df['imdb_id'] = wiki_movies_df['imdb_link'].str.extract(r'(tt\\d{7})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e2b6a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [Execute]\n",
    "\n",
    "# Now we can drop any duplicates of IMDb IDs by using the drop_dupilcates() method.\n",
    "# To specify that we only want to consider the IMBd ID, use the subset argument,\n",
    "# and set inplace equal to True so that the operation is performed on the slected dataframe.\n",
    "# Otherwise, the operation would return an edited dataframe that would need to be saved to a new variable.\n",
    "# We also want to see the new number of rows and how many rows were dropped.\n",
    "\n",
    "# Code to extract the IMBd ID \n",
    "wiki_movies_df['imdb_id'] = wiki_movies_df['imdb_link'].str.extract(r'(tt\\d{7})')\n",
    "print(len(wiki_movies_df))\n",
    "wiki_movies_df.drop_duplicates(subset='imdb_id', inplace=True)\n",
    "print(len(wiki_movies_df))\n",
    "wiki_movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Mostly Null Columns\n",
    "# Now that we've consolidated redundant columns, we want to see which columns don't contain much useful data.\n",
    "# Since this is scraped data, it's possible many columns are mostly null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66074c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Inspect]\n",
    "\n",
    "# Get the count of null values for each column by using list comprehension\n",
    "[[column,wiki_movies_df[column].isnull().sum()] for column in wiki_movies_df.columns]\n",
    "\n",
    "# Could also use a for loop and a print statement\n",
    "# Either way, we can see about half the columns have more than 6,000 null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312f515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# We could remove them by hand, but it's better to do it programmatically to make sure we don't miss any.\n",
    "# Let's make a list of columns that have less than 90% null values and use those to trim down our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b671bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns that have less than 90% null values and use those to trim down our dataset.\n",
    "[column for column in wiki_movies_df.columns if wiki_movies_df[column].isnull().sum() < len(wiki_movies_df) * 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387a65fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above gave us the columns that we want to keep, which we can select from out Pandas Dataframe as follows:\n",
    "wiki_columns_to_keep = [column for column in wiki_movies_df.columns if wiki_movies_df[column].isnull().sum() < len(wiki_movies_df) * 0.9]\n",
    "wiki_movies_df = wiki_movies_df[wiki_columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f68bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may have noticed that the \"alt_titles\" column we created earlier was deleted by this bit of code.\n",
    "# It might feel like all that work we did was futile, but it's not.\n",
    "# It's possible that all of the alternate title columns individually had less than 10% non-null values,\n",
    "# but collectively had enough data to keep. We wouldn't know that unless we put in that work.\n",
    "# This is normal for data cleaning because it's an iterative process.\n",
    "# Sometimes the hard work you put in doesn't seem to make it to the final product, but don't worry, it's in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c743ebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Inspect]\n",
    "\n",
    "# First, we need to identify which columns need to be converted.\n",
    "# wiki_movies_df.dtypes will display the data type for each column.\n",
    "\n",
    "# Identify which columns need to be converted\n",
    "wiki_movies_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d889a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# The apply function takes and applies another function to every single value of a Pandas series.\n",
    "# Looking through the data, column by column, we see that:\n",
    "\n",
    "    # Box office should be numeric.\n",
    "    # Budget should be numeric.\n",
    "    # Release date should be a date object.\n",
    "    # Running time should be numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba1b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll start on the box office data, which should give us code that we can reuse and\n",
    "# tweak for the budget data since they're both currency.\n",
    "# It will be helpful to only look at rows where box office data is defined,\n",
    "# so first we'll make a data series that drops missing values with the following:\n",
    "\n",
    "box_office = wiki_movies_df['Box office'].dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db8ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expressions only work on strings,\n",
    "# so we'll need to make sure all of the box office data is entered as a string.\n",
    "# By using the map() method, we can see which values are not strings.\n",
    "# First, make a is_not_a_string() function:\n",
    "\n",
    "def is_not_a_string(x):\n",
    "    return type(x) != str\n",
    "\n",
    "# Then add the following:\n",
    "box_office[box_office.map(is_not_a_string)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8628d744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having to create a new function every time we want to use the map() method is cumbersome\n",
    "# and interrupts the readability of our code. What we want is a stripped-down, one-line way of writing our functions.\n",
    "# Also, we don't need to use it ever again outside of our map() call, so we don't need to give it a name.\n",
    "# If you think we're talking about types of functions that will work here, you're right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, this is what lambda functions are made for.\n",
    "# Instead of creating a new function with a block of code and the def keyword,\n",
    "# we can create an anonymous lambda function right inside the map() call.\n",
    "\n",
    "# They use the following syntax:\n",
    "# lambda arguments: expression\n",
    "\n",
    "# So the lambda function version of is_not_a_string() is:\n",
    "# lambda x: type(x) != str\n",
    "\n",
    "# We can update our map() call to use the lambda function directly instead of using is_not_a_string():\n",
    "box_office[box_office.map(lambda x: type(x) != str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753f5819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the output, we can see that there are quite a few data points that are stored as lists.\n",
    "# There is a join() string method that concatenates list items into one string;\n",
    "# however, we can't just type join(some_list) because the join() method belongs to string objects.\n",
    "# We need to make a separator string and then call the join() method on it. For example, the code would be:\n",
    "\n",
    "# some_list = ['One','Two','Three']\n",
    "# 'Mississippi'.join(some_list)\n",
    "\n",
    "# The outputs would be:\n",
    "# 'OneMississippiTwoMississippiThree'\n",
    "\n",
    "# We'll use a simple space as our joining character and apply the join()\n",
    "# function only when our data points are lists.\n",
    "# The code looks like the following:\n",
    "\n",
    "box_office = box_office.apply(lambda x: ' '.join(x) if type(x) == list else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ef37d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a built-in Python module for regular expressions: re.\n",
    "# We'll need to import that library, so add the line below to the first cell,\n",
    "# with the other import statements, and rerun the cell.\n",
    "\n",
    "# import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afce0181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character Types:\n",
    "\n",
    "# \\d - matches any digit from 0 to 9\n",
    "# \\D - matches any non-digit character\n",
    "# \\w - matches a word character (a letter, digit, or underscore)\n",
    "# \\W - matches any non-word character (anything other than a letter, digit, or underscore, such as spaces and punctuation)\n",
    "# \\s - matches any whitespace character (including spaces, tabs, and newlines)\n",
    "# \\S - Matches any non-whitespace characters\n",
    "# [ ...] Character Set - Matches any characters inside the brackets. Can specify ranges of characters as well.\n",
    "# [^ … ] Negative Character - Set Matches anything not inside the brackets\n",
    "# . Wildcard - Matches any character (except a newline)\n",
    "# * Matches 0 or more times\n",
    "# + Matches 1 or more times\n",
    "# ? Matches 0 or 1 time\n",
    "# {#} Matches a specific number of times\n",
    "# {#,} Matches at least a specific number of times\n",
    "# {#,#} Matches within a specific range of times\n",
    "# | Alternation - Matches either the expression before or the expression after\n",
    "# ^ Start of the string\n",
    "# $ End of the string\n",
    "# </code> Escape Character - Escapes the next character to be treated as a literal character\n",
    "# ( … ) Capture Group - Identifies matches that should be extracted\n",
    "# (?: … ) Non-Capturing Group - Identifies matches that should not be extracted\n",
    "# (?! … ) Negative Lookahead Group - Identifies expressions that negate earlier matches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a0566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# For the first form, our pattern match string will include six elements in the following order:\n",
    "# 1) A dollar sign\n",
    "# 2) An arbitrary (but non-zero) number of digits\n",
    "# 3) An optional decimal point\n",
    "# 4) An arbitrary (but possibly zero) number of more digits\n",
    "# 5) A space (maybe more than one)\n",
    "# 6) The word \"million\" or \"billion\"\n",
    "\n",
    "form_one = r'\\$\\d+\\.?\\d*\\s*[mb]illion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d8b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Inspect]\n",
    "\n",
    "# We'll use the str.contains() method on box_office.\n",
    "# To ignore whether letters are uppercase or lowercase,\n",
    "# add an argument called flags, and set it equal to re.IGNORECASE.\n",
    "# In case the data is not a string, we'll add the na=False argument to parse the non-string data to False.\n",
    "# Finally, we can call the sum() method to count up the total number that return True.\n",
    "# The code should look like the following:\n",
    "\n",
    "box_office.str.contains(form_one, flags=re.IGNORECASE, na=False).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad55c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# Next, we'll match the numbers of our second form,\"$123,456,789.\"\n",
    "# In words, our pattern match string will include the following elements:\n",
    "\n",
    "# 1) A dollar sign\n",
    "# 2) A group of one to three digits\n",
    "# 3) At least one group starting with a comma and followed by exactly three digits\n",
    "\n",
    "form_two = r'\\$\\d{1,3}(?:,\\d{3})+'\n",
    "box_office.str.contains(form_two, flags=re.IGNORECASE, na=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ea8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Inspect]\n",
    "\n",
    "# To make our code easier to understand, we'll create two Boolean Series called matches_form_one and matches_form_two,\n",
    "# and then select the box office values that don't match either. \n",
    "\n",
    "matches_form_one = box_office.str.contains(form_one, flags=re.IGNORECASE, na=False)\n",
    "matches_form_two = box_office.str.contains(form_two, flags=re.IGNORECASE, na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ad9062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will throw an error!\n",
    "box_office[(not matches_form_one) and (not matches_form_two)]\n",
    "\n",
    "# The code above will give you a ValueError with the explanation \"The truth value of a Series is ambiguous.\"\n",
    "# (Unfortunately, the meaning of that error is also ambiguous.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00babbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead, Pandas has element-wise logical operators:\n",
    "\n",
    "# The element-wise negation operator is the tilde: ~ (similar to \"not\")\n",
    "# The element-wise logical \"and\" is the ampersand: &\n",
    "# The element-wise logical \"or\" is the pipe: |\n",
    "# The code we want to use is as follows:\n",
    "\n",
    "box_office[~matches_form_one & ~matches_form_two]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d24dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# We can fix our pattern matches to capture more values by addressing these issues:\n",
    "\n",
    "# 1) Some values have spaces in between the dollar sign and the number.\n",
    "# form_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illion'\n",
    "# form_two = r'\\$\\s*\\d{1,3}(?:,\\d{3})+'\n",
    "\n",
    "# 2) Some values use a period as a thousands separator, not a comma.\n",
    "# form_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+'\n",
    "\n",
    "# The results will also match values like 1.234 billion, but we're trying to change raw numbers like $123.456.789.\n",
    "# We don't want to capture any values like 1.234 billion,\n",
    "# so we need to add a negative lookahead group that looks ahead for \"million\" or \"billion\"\n",
    "# after the number and rejects the match if it finds those strings.\n",
    "# Don't forget the space! The new form should look like this:\n",
    "\n",
    "form_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)'\n",
    "\n",
    "# 3) Some values are given as a range.\n",
    "box_office = box_office.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)\n",
    "\n",
    "# 4) \"Million\" is sometimes misspelled as \"millon.\"\n",
    "form_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4285314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Execute]\n",
    "\n",
    "# Extract and convert the box office values\n",
    "\n",
    "# Now that we've got expressions to match almost all the box office values,\n",
    "# we'll use them to extract only the parts of the strings that match.\n",
    "# We do this with the str.extract() method. This method also takes in a regular expression string,\n",
    "# but it returns a DataFrame where every column is the data that matches a capture group.\n",
    "\n",
    "# The f-string f'{form_one}|{form_two}' will create a regular expression that matches either form_one or form_two,\n",
    "# so we just need to put the whole thing in parentheses to create a capture group.\n",
    "# Our final string will be f'({form_one}|{form_two})',\n",
    "# and the full line of code to extract the data follows:\n",
    "\n",
    "box_office.str.extract(f'({form_one}|{form_two})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3264063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need a function to turn the extracted values into a numeric value.\n",
    "# We'll call it parse_dollars, and parse_dollars will take in a string and return a floating-point number. \n",
    "# We'll start by making a skeleton function with comments explaining each step, and then fill in the steps with actual code.\n",
    "\n",
    "# def parse_dollars(s):\n",
    "    # if s is not a string, return NaN\n",
    "\n",
    "    # if input is of the form $###.# million\n",
    "\n",
    "        # remove dollar sign and \" million\"\n",
    "\n",
    "        # convert to float and multiply by a million\n",
    "\n",
    "        # return value\n",
    "\n",
    "    # if input is of the form $###.# billion\n",
    "\n",
    "        # remove dollar sign and \" billion\"\n",
    "\n",
    "        # convert to float and multiply by a billion\n",
    "\n",
    "        # return value\n",
    "\n",
    "    # if input is of the form $###,###,###\n",
    "\n",
    "        # remove dollar sign and commas\n",
    "\n",
    "        # convert to float\n",
    "\n",
    "        # return value\n",
    "\n",
    "    # otherwise, return NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1999c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we're working directly with strings, we'll use the re module to access the regular expression functions.\n",
    "# We'll use re.match(pattern, string) to see if our string matches a pattern.\n",
    "# To start, we'll make some small alterations to the forms we defined, splitting the million and billion matches from form one.\n",
    "\n",
    "# def parse_dollars(s):\n",
    "    # if s is not a string, return NaN\n",
    "    # if type(s) != str:\n",
    "        # return np.nan\n",
    "\n",
    "    # if input is of the form $###.# million\n",
    "    # if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*milli?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "        # remove dollar sign and \" million\"\n",
    "\n",
    "        # convert to float and multiply by a million\n",
    "\n",
    "        # return value\n",
    "\n",
    "    # if input is of the form $###.# billion\n",
    "    # elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*billi?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "        # remove dollar sign and \" billion\"\n",
    "\n",
    "        # convert to float and multiply by a billion\n",
    "\n",
    "        # return value\n",
    "\n",
    "    # if input is of the form $###,###,###\n",
    "    # elif re.match(r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)', s, flags=re.IGNORECASE):\n",
    "\n",
    "        # remove dollar sign and commas\n",
    "\n",
    "        # convert to float\n",
    "\n",
    "        # return value\n",
    "\n",
    "    # otherwise, return NaN\n",
    "    # else:\n",
    "        # return np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d7591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we'll use re.sub(pattern, replacement_string, string) \n",
    "# to remove dollar signs, spaces, commas, and letters, if necessary.\n",
    "\n",
    "# def parse_dollars(s):\n",
    "    # if s is not a string, return NaN\n",
    "    # if type(s) != str:\n",
    "        # return np.nan\n",
    "\n",
    "    # if input is of the form $###.# million\n",
    "    # if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*milli?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "        # remove dollar sign and \" million\"\n",
    "        # s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "        # convert to float and multiply by a million\n",
    "\n",
    "        # return value\n",
    "\n",
    "    # if input is of the form $###.# billion\n",
    "    # elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*billi?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "        # remove dollar sign and \" billion\"\n",
    "        # s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "        # convert to float and multiply by a billion\n",
    "\n",
    "        # return value\n",
    "\n",
    "    # if input is of the form $###,###,###\n",
    "    # elif re.match(r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)', s, flags=re.IGNORECASE):\n",
    "\n",
    "        # remove dollar sign and commas\n",
    "        # s = re.sub('\\$|,','', s)\n",
    "\n",
    "        # convert to float\n",
    "\n",
    "        # return value\n",
    "\n",
    "    # otherwise, return NaN\n",
    "    # else:\n",
    "        # return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d449846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, convert all the strings to floats, multiply by the right amount, and return the value.\n",
    "def parse_dollars(s):\n",
    "    # if s is not a string, return NaN\n",
    "    if type(s) != str:\n",
    "        return np.nan\n",
    "\n",
    "    # if input is of the form $###.# million\n",
    "    if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*milli?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "        # remove dollar sign and \" million\"\n",
    "        s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "        # convert to float and multiply by a million\n",
    "        value = float(s) * 10**6\n",
    "\n",
    "        # return value\n",
    "        return value\n",
    "\n",
    "    # if input is of the form $###.# billion\n",
    "    elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*billi?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "        # remove dollar sign and \" billion\"\n",
    "        s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "        # convert to float and multiply by a billion\n",
    "        value = float(s) * 10**9\n",
    "\n",
    "        # return value\n",
    "        return value\n",
    "\n",
    "    # if input is of the form $###,###,###\n",
    "    elif re.match(r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)', s, flags=re.IGNORECASE):\n",
    "\n",
    "        # remove dollar sign and commas\n",
    "        s = re.sub('\\$|,','', s)\n",
    "\n",
    "        # convert to float\n",
    "        value = float(s)\n",
    "\n",
    "        # return value\n",
    "        return value\n",
    "\n",
    "    # otherwise, return NaN\n",
    "    else:\n",
    "        return np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5804289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have everything we need to parse the box office values to numeric values.\n",
    "\n",
    "# First, we need to extract the values from box_office using str.extract.\n",
    "# Then we'll apply parse_dollars to the first column in the DataFrame returned by str.extract,\n",
    "# which in code looks like the following:\n",
    "\n",
    "wiki_movies_df['box_office'] = box_office.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8784e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_movies_df['box_office']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_movies_df.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734380d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We no longer need the Box Office column, so we'll just drop it:\n",
    "wiki_movies_df.drop('Box office', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e39503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a budget variable with the following code:\n",
    "budget = wiki_movies_df['Budget'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c1f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert any lists to strings:\n",
    "budget = budget.map(lambda x: ' '.join(x) if type(x) == list else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad809032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then remove any values between a dollar sign and a hyphen (for budgets given in ranges):\n",
    "budget = budget.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e7a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same pattern matches that you created to parse the box office data,\n",
    "# and apply them without modifications to the budget data. Then, look at what's left.\n",
    "\n",
    "matches_form_one = budget.str.contains(form_one, flags=re.IGNORECASE, na=False)\n",
    "matches_form_two = budget.str.contains(form_two, flags=re.IGNORECASE, na=False)\n",
    "budget[~matches_form_one & ~matches_form_two]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cab7225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That parsed almost all of the budget data.\n",
    "# However, there's a new issue with the budget data: citation references (the numbers in square brackets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea313242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# We can remove those fairly easily with a regular expression.\n",
    "# Remove the citation references with the following:\n",
    "\n",
    "budget = budget.str.replace(r'\\[\\d+\\]\\s*', '')\n",
    "budget[~matches_form_one & ~matches_form_two]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a3cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything is now ready to parse the budget values.\n",
    "# We can copy the line of code we used to parse the box office values, changing \"box_office\" to \"budget\":\n",
    "wiki_movies_df['budget'] = budget.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b17d4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also drop the original Budget column.\n",
    "wiki_movies_df.drop('Budget', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ddf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse Release Date\n",
    "# Parsing the release date will follow a similar pattern to parsing box office and budget, but with different forms.\n",
    "#First, make a variable that holds the non-null values of Release date in the DataFrame, converting lists to strings:\n",
    "release_date = wiki_movies_df['Release date'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02113784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The forms we'll be parsing are:\n",
    "\n",
    "# Full month name, one- to two-digit day, four-digit year (i.e., January 1, 2000)\n",
    "# Four-digit year, two-digit month, two-digit day, with any separator (i.e., 2000-01-01)\n",
    "# Full month name, four-digit year (i.e., January 2000)\n",
    "# Four-digit year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3820d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One way to parse those forms is with the following:\n",
    "date_form_one = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s[123]?\\d,\\s\\d{4}'\n",
    "date_form_two = r'\\d{4}.[01]\\d.[0123]\\d'\n",
    "date_form_three = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{4}'\n",
    "date_form_four = r'\\d{4}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7360d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})', flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ac7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_movies_df['release_date'] = pd.to_datetime(release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})')[0], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78285bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse Running Time\n",
    "# First, make a variable that holds the non-null values of Release date in the DataFrame, converting lists to strings:\n",
    "running_time = wiki_movies_df['Running time'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84711d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Inspect]\n",
    "# It looks like most of the entries just look like \"100 minutes.\"\n",
    "# Let's see how many running times look exactly like that by using string boundaries.\n",
    "running_time.str.contains(r'^\\d*\\s*minutes$', flags=re.IGNORECASE, na=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c5cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above code returns 6,528 entries. Let's get a sense of what the other 366 entries look like.\n",
    "\n",
    "running_time[running_time.str.contains(r'^\\d*\\s*minutes$', flags=re.IGNORECASE, na=False) != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ea17b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make this more general by only marking the beginning of the string,\n",
    "# and accepting other abbreviations of \"minutes\" by only searching up to the letter \"m.\"\n",
    "running_time.str.contains(r'^\\d*\\s*m', flags=re.IGNORECASE, na=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b57b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That accounts for 6,877 entries. The remaining 17 follow:\n",
    "running_time[running_time.str.contains(r'^\\d*\\s*m', flags=re.IGNORECASE, na=False) != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e05d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# We can match all of the hour + minute patterns with one regular expression pattern. Our pattern follows:\n",
    "\n",
    "# 1) Start with one or more digits.\n",
    "# 2) Have an optional space after the digit and before the letter \"h.\"\n",
    "# 3) Capture all the possible abbreviations of \"hour(s).\" To do this, we'll make every letter in \"hours\" optional except the \"h.\"\n",
    "# 4) Have an optional space after the \"hours\" marker.\n",
    "# 5) Have an optional number of digits for minutes.\n",
    "\n",
    "# As a pattern, this looks like \"\\d+\\s*ho?u?r?s?\\s*\\d*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a868f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Execute]\n",
    "\n",
    "running_time_extract = running_time.str.extract(r'(\\d+)\\s*ho?u?r?s?\\s*(\\d*)|(\\d+)\\s*m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40684c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunately, this new DataFrame is all strings, we'll need to convert them to numeric values.\n",
    "# Because we may have captured empty strings, we'll use the to_numeric() method and set the errors argument to 'coerce'.\n",
    "# Coercing the errors will turn the empty strings into Not a Number (NaN),\n",
    "# then we can use fillna() to change all the NaNs to zeros.\n",
    "\n",
    "running_time_extract = running_time_extract.apply(lambda col: pd.to_numeric(col, errors='coerce')).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f04b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can apply a function that will convert the hour capture groups and minute capture groups to minutes\n",
    "# if the pure minutes capture group is zero,and save the output to wiki_movies_df:\n",
    "wiki_movies_df['running_time'] = running_time_extract.apply(lambda row: row[0]*60 + row[1] if row[2] == 0 else row[2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49239b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we can drop Running time from the dataset with the following code:\n",
    "wiki_movies_df.drop('Running time', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7fd745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Look at the Movie Metadata\n",
    "\n",
    "# [Inspect]\n",
    "# Because the Kaggle data came in as a CSV, one of the first things we want to check is that all\n",
    "# of the columns came in as the correct data types.\n",
    "kaggle_metadata.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c6fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll just go down the list and convert the data types for each of the six columns that need to be converted.\n",
    "# Before we convert the \"adult\" and \"video\" columns, we want to check that all the values are either True or False.\n",
    "\n",
    "kaggle_metadata['adult'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Bad Data\n",
    "# To remove the bad data, use the following:\n",
    "\n",
    "kaggle_metadata[~kaggle_metadata['adult'].isin(['True','False'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b766da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code will keep rows where the adult column is False, and then drop the adult column.\n",
    "kaggle_metadata = kaggle_metadata[kaggle_metadata['adult'] == 'False'].drop('adult',axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c17788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we'll look at the values of the video column:\n",
    "kaggle_metadata['video'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d060aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Data Types\n",
    "\n",
    "# [Execute]\n",
    "\n",
    "# Great, there are only False and True values. We can convert video fairly easily.\n",
    "# To convert, use the following code:\n",
    "kaggle_metadata['video'] == 'True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585fe0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above code creates the Boolean column we want. We just need to assign it back to video:\n",
    "kaggle_metadata['video'] = kaggle_metadata['video'] == 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d6509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# For the numeric columns, we can just use the to_numeric() method from Pandas.\n",
    "# We'll make sure the errors= argument is set to 'raise',\n",
    "# so we'll know if there's any data that can't be converted to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296570d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Execute]\n",
    "\n",
    "kaggle_metadata['budget'] = kaggle_metadata['budget'].astype(int)\n",
    "kaggle_metadata['id'] = pd.to_numeric(kaggle_metadata['id'], errors='raise')\n",
    "kaggle_metadata['popularity'] = pd.to_numeric(kaggle_metadata['popularity'], errors='raise')\n",
    "\n",
    "# This code runs without errors, so everything converted fine.\n",
    "# Finally, we need to convert release_date to datetime.\n",
    "# Luckily, Pandas has a built-in function for that as well: to_datetime()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d85d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# Since release_date is in a standard format, to_datetime() will convert it without any fuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6d5065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Execute]\n",
    "\n",
    "kaggle_metadata['release_date'] = pd.to_datetime(kaggle_metadata['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e60851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasonability Checks on Rating Data\n",
    "\n",
    "# [Inspect]\n",
    "\n",
    "# Lastly, we'll take a look at the ratings data. We'll use the info() method on the DataFrame.\n",
    "# Since the ratings dataset has so many rows, we need to set the null_counts option to True.\n",
    "\n",
    "ratings.info(null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f56e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our own analysis, we won't be using the timestamp column;\n",
    "# however, we will be storing the rating data as its own table in SQL, so we'll need to convert it to a datetime data type.\n",
    "# From the MovieLens documentation, the timestamp is the number of seconds since midnight of January 1, 1970.\n",
    "\n",
    "# Storing time values as a data type is difficult, and there are many, many standards out there for time values.\n",
    "# Some store time values as text strings, like the ISO format \"1955-11-05T12:00:00,\"\n",
    "# but then calculating the difference between two time values is complicated and computationally expensive.\n",
    "# The Unix time standard stores points of time as integers,\n",
    "# specifically as the number of seconds that have elapsed since midnight of January 1, 1970. This is known as the Unix epoch.\n",
    "# There are other epochs in use, but the Unix epoch is by far the most widespread.\n",
    "\n",
    "\n",
    "# We'll specify in to_datetime() that the origin is 'unix' and the time unit is seconds.\n",
    "pd.to_datetime(ratings['timestamp'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668b5199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# These dates don't seem outlandish—the years are within expected bounds,\n",
    "# and there appears to be some consistency from one entry to the next. Since the output looks reasonable,\n",
    "# assign it to the timestamp column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07afaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Execute]\n",
    "ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b0d32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Inspect]\n",
    "\n",
    "# Finally, we'll look at the statistics of the actual ratings and see if there are any glaring errors.\n",
    "# A quick, easy way to do this is to look at a histogram of the rating distributions,\n",
    "# and then use the describe() method to print out some stats on central tendency and spread.\n",
    "\n",
    "# A histogram is a bar chart that displays how often a data point shows up in the data.\n",
    "# A histogram is a quick, visual way to get a sense of how a dataset is distributed.\n",
    "# Your code should look like this:\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "ratings['rating'].plot(kind='hist')\n",
    "ratings['rating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8f4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# That seems to make sense. People are more likely to give whole number ratings than half,\n",
    "# which explains the spikes in the histogram. The median score is 3.5,\n",
    "# the mean is 3.53, and all the ratings are between 0 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5d5eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Execute]\n",
    "# The ratings dataset looks good to go, which means we're done with the first half of the Transform step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddac88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Inspect]\n",
    "\n",
    "# One of the things we always want to look out for after we've merged data is redundant columns.\n",
    "# Print out a list of the columns so we can identify which ones are redundant.# \n",
    "# We'll use the suffixes parameter to make it easier to identify which table each column came from. Here's what your code should look like:\n",
    "\n",
    "movies_df = pd.merge(wiki_movies_df, kaggle_metadata, on='imdb_id', suffixes=['_wiki','_kaggle'])\n",
    "movies_df\n",
    "\n",
    "# There are seven pairs of columns that have redundant information.\n",
    "# We'll look at each pair of columns and decide how to handle the data.\n",
    "# There are a few options when dealing with redundant data. We'll consider two.\n",
    "# The simplest is to just drop one of the competing columns, but sometimes that means a loss of good information.\n",
    "# Sometimes, one column will have data where the other has missing data, and vice versa.\n",
    "# In that case, we'd want the other option: fill in the gaps using both columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09efeab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# We'll fill in the resolution to each pair as we go along.\n",
    "# We'll hold off on implementing the resolutions until we make a decision for each pair because if we did,\n",
    "# we might inadvertently remove data that could be helpful in making a later decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88806f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may find it helpful to include a table like this in your Jupyter Notebook\n",
    "# that documents the decisions made and the justifications for them.\n",
    "# Unfortunately, markdown doesn't support formatting tables.\n",
    "# One way to work around that is to just write your text down as comments in a code cell.\n",
    "\n",
    "\n",
    "\n",
    "# Competing data:\n",
    "# Wiki                     Movielens                Resolution\n",
    "#--------------------------------------------------------------------------\n",
    "# title_wiki               title_kaggle\n",
    "# running_time             runtime\n",
    "# budget_wiki              budget_kaggle\n",
    "# box_office               revenue\n",
    "# release_date_wiki        release_date_kaggle\n",
    "# Language                 original_language\n",
    "# Production company(s)    production_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c610c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title\n",
    "\n",
    "# First, just take a quick look at some of the titles.\n",
    "movies_df[['title_wiki','title_kaggle']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c33d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# They both seem pretty consistent, which we'd expect. Look at the rows where the titles don't match.\n",
    "movies_df[movies_df['title_wiki'] != movies_df['title_kaggle']][['title_wiki','title_kaggle']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a13f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both options look pretty good, but the Kaggle data looks just a little bit more consistent. \n",
    "# Let's confirm there aren't any missing titles in the Kaggle data with the following code:\n",
    "\n",
    "# Show any rows where title_kaggle is empty\n",
    "movies_df[(movies_df['title_kaggle'] == '') | (movies_df['title_kaggle'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d2bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "# No results were returned, so we can just drop the Wikipedia titles. Note that for now, we’re merely noting the resolution.\n",
    "\n",
    "\n",
    "# Competing data:\n",
    "# Wiki                     Movielens                Resolution\n",
    "#--------------------------------------------------------------------------\n",
    "# title_wiki               title_kaggle             Drop Wikipedia\n",
    "# running_time             runtime\n",
    "# budget_wiki              budget_kaggle\n",
    "# box_office               revenue\n",
    "# release_date_wiki        release_date_kaggle\n",
    "# Language                 original_language\n",
    "# Production company(s)    production_companies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e827b7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime\n",
    "\n",
    "# [Inspect]\n",
    "\n",
    "# Next, look at running_time versus runtime.\n",
    "# A scatter plot is a great way to give us a sense of how similar the columns are to each other.\n",
    "# If the two columns were exactly the same, we'd see a scatter plot of a perfectly straight line.\n",
    "# Any wildly different values will show up as dots far from that central line, and if one column is missing data,\n",
    "# those values will fall on the x-axis or y-axis.\n",
    "\n",
    "# Because we're dealing with merged data, we should expect there to be missing values.\n",
    "# Scatter plots won't show null values, so we need to fill them in with zeros when we're making our plots\n",
    "# to get the whole picture.\n",
    "\n",
    "# The following code will fill in missing values with zero and make the scatter plot:\n",
    "\n",
    "movies_df.fillna(0).plot(x='running_time', y='runtime', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7b407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# Notice that there are more data points on the origin of the Y axis than on the origin of the X axis.\n",
    "# Since the X axis is Wikipedia and the Y axis is Kaggle,\n",
    "# this means there are more missing entries in the Wikipedia data set than in the Kaggle data set.\n",
    "# Also, most of the runtimes are pretty close to each other but the Wikipedia data has some outliers,\n",
    "# so the Kaggle data is probably a better choice here.\n",
    "# However, we can also see from the scatter plot that there are movies where Kaggle has 0\n",
    "# for the runtime but Wikipedia has data, so we'll fill in the gaps with Wikipedia data.\n",
    "\n",
    "# Competing data:\n",
    "# Wiki                     Movielens                Resolution\n",
    "#--------------------------------------------------------------------------\n",
    "# title_wiki               title_kaggle             Drop Wikipedia\n",
    "# running_time             runtime                  Keep Kaggle; fill in zeros with Wikipedia data.\n",
    "# budget_wiki              budget_kaggle\n",
    "# box_office               revenue\n",
    "# release_date_wiki        release_date_kaggle\n",
    "# Language                 original_language\n",
    "# Production company(s)    production_companies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0288c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Budget\n",
    "\n",
    "# [Inspect]\n",
    "# Since budget_wiki and budget_kaggle are numeric, we'll make another scatter plot to compare the values:\n",
    "movies_df.fillna(0).plot(x='budget_wiki',y='budget_kaggle', kind='scatter')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75023ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some questions to consider when interpreting this scatter plot:\n",
    "\n",
    "# 1) Which dataset seems to have more outliers?\n",
    "\n",
    "# 2) Which dataset seems to have more missing data points?\n",
    "\n",
    "# 3) If we were to fill in the missing data points of one set with the other,\n",
    "# which would be more likely to give us consistent data?\n",
    "\n",
    "# 4) Is it better to start with a base of consistent data and fill in missing points with possible outliers?\n",
    "# Or is it better to start with a base of data with outliers and fill in missing points with more consistent data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f15241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# The Wikipedia data appears to have more outliers compared to the Kaggle data.\n",
    "# However, there are quite a few movies with no data in the Kaggle column,\n",
    "# while Wikipedia does have budget data.# Therefore, we'll fill in the gaps with Wikipedia's data.\n",
    "\n",
    "# Competing data:\n",
    "# Wiki                     Movielens                Resolution\n",
    "#--------------------------------------------------------------------------\n",
    "# title_wiki               title_kaggle             Drop Wikipedia\n",
    "# running_time             runtime                  Keep Kaggle; fill in zeros with Wikipedia data.\n",
    "# budget_wiki              budget_kaggle            Keep Kaggle; fill in zeros with Wikipedia data.\n",
    "# box_office               revenue\n",
    "# release_date_wiki        release_date_kaggle\n",
    "# Language                 original_language\n",
    "# Production company(s)    production_companies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35147db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Office\n",
    "\n",
    "# [Inspect]\n",
    "# The box_office and revenue columns are numeric, so we'll make another scatter plot.\n",
    "\n",
    "movies_df.fillna(0).plot(x='box_office', y='revenue', kind='scatter')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8892d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That looks pretty close, but we might be getting thrown off by the scale of that large data point.\n",
    "# Let's look at the scatter plot for everything less than $1 billion in box_office.\n",
    "\n",
    "movies_df.fillna(0)[movies_df['box_office'] < 10**9].plot(x='box_office', y='revenue', kind='scatter')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1283de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# This looks similar to what we've seen for budget, so we'll make the same decision:\n",
    "# keep the Kaggle data, but fill in the zeros with Wikipedia data.\n",
    "\n",
    "# Competing data:\n",
    "# Wiki                     Movielens                Resolution\n",
    "#--------------------------------------------------------------------------\n",
    "# title_wiki               title_kaggle             Drop Wikipedia\n",
    "# running_time             runtime                  Keep Kaggle; fill in zeros with Wikipedia data.\n",
    "# budget_wiki              budget_kaggle            Keep Kaggle; fill in zeros with Wikipedia data.\n",
    "# box_office               revenue                  Keep Kaggle; fill in zeros with Wikipedia data.\n",
    "# release_date_wiki        release_date_kaggle\n",
    "# Language                 original_language\n",
    "# Production company(s)    production_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1398a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release Date\n",
    "# [Inspect]\n",
    "\n",
    "# For release_date_wiki and release_date_kaggle, we can't directly make a scatter plot,\n",
    "# because the scatter plot only works on numeric data. However, there's a tricky workaround that we can use.\n",
    "# We'll use the regular line plot (which can plot date data), and change the style to only put dots by adding style='.'\n",
    "# to the plot() method:\n",
    "\n",
    "movies_df[['release_date_wiki','release_date_kaggle']].plot(x='release_date_wiki', y='release_date_kaggle', style='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb37b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should investigate that wild outlier around 2006.\n",
    "# We're just going to choose some rough cutoff dates to single out that one movie.\n",
    "# We'll look for any movie whose release date according to Wikipedia is after 1996,\n",
    "# but whose release date according to Kaggle is before 1965.\n",
    "# Here's what your code should look like:\n",
    "\n",
    "movies_df[(movies_df['release_date_wiki'] > '1996-01-01') & (movies_df['release_date_kaggle'] < '1965-01-01')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd51de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the output, it looks like somehow The Holiday in the Wikipedia data got merged with From Here to Eternity.\n",
    "# We'll have to drop that row from our DataFrame.\n",
    "# We'll get the index of that row with the following:\n",
    "\n",
    "movies_df[(movies_df['release_date_wiki'] > '1996-01-01') & (movies_df['release_date_kaggle'] < '1965-01-01')].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d4025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Execute]\n",
    "\n",
    "# Then we can drop that row like this:\n",
    "\n",
    "movies_df = movies_df.drop(movies_df[(movies_df['release_date_wiki'] > '1996-01-01') & (movies_df['release_date_kaggle'] < '1965-01-01')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4e4368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, see if there are any null values:\n",
    "\n",
    "movies_df[movies_df['release_date_wiki'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945bd01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan] \n",
    "# The Wikipedia data is missing release dates for 11 movies.\n",
    "# Wikipedia data is missing but the Kaggle data isn't missing any release dates.\n",
    "# In this case, we'll just drop the Wikipedia data.\n",
    "\n",
    "# Competing data:\n",
    "# Wiki                     Movielens                Resolution\n",
    "#--------------------------------------------------------------------------\n",
    "# title_wiki               title_kaggle             Drop Wikipedia\n",
    "# running_time             runtime                  Keep Kaggle; fill in zeros with Wikipedia data.\n",
    "# budget_wiki              budget_kaggle            Keep Kaggle; fill in zeros with Wikipedia data.\n",
    "# box_office               revenue                  Keep Kaggle; fill in zeros with Wikipedia data.\n",
    "# release_date_wiki        release_date_kaggle      Drop Wikipedia\n",
    "# Language                 original_language\n",
    "# Production company(s)    production_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854d7f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language\n",
    "\n",
    "\n",
    "# [Inspect] \n",
    "\n",
    "# For the language data, we'll compare the value counts of each. \n",
    "# However, consider the following code:\n",
    "\n",
    "# movies_df['Language'].value_counts()\n",
    "\n",
    "# This code throws an error because some of the language data points are stored as lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need to worry about what hashing is right now, but if you're curious,\n",
    "# hashing is a clever computer science trick that can be used to speed up algorithms like getting value counts.\n",
    "# Hashing converts values, even arbitrarily long strings, to a limited space of numerical values.\n",
    "# We'll talk about hashing more when we get to machine learning, but for now,\n",
    "# the important part is that Python creates hash values when new objects are created if they are immutable.\n",
    "# Since mutable objects can have their values change after being created, the values might change and not match the hash,\n",
    "# so Python just refuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbd6100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to convert the lists in Language to tuples so that the value_counts() method will work.\n",
    "# See the following code:\n",
    "\n",
    "movies_df['Language'].apply(lambda x: tuple(x) if type(x) == list else x).value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b1c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Kaggle data, there are no lists, so we can just run value_counts() on it.\n",
    "\n",
    "movies_df['original_language'].value_counts(dropna=False)\n",
    "\n",
    "# There's a trade-off here between the Wikipedia language data and the Kaggle language data.\n",
    "# While the Wikipedia data has more information about multiple languages,\n",
    "# the Kaggle data is already in a consistent and usable format.\n",
    "# Parsing the Wikipedia data may create too many difficulties to make it worthwhile, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abca89bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan] \n",
    "# This is another judgment call; there's no clear-cut answer here.\n",
    "# However, for better or for worse, decisions that save time are usually the ones that win, so we'll use the Kaggle data here.\n",
    "\n",
    "# Competing data:\n",
    "# Wiki                     Movielens                Resolution\n",
    "#--------------------------------------------------------------------------\n",
    "# title_wiki               title_kaggle             Drop Wikipedia.\n",
    "# running_time             runtime                  Keep Kaggle; fill in zeros with Wikipedia data.\n",
    "# budget_wiki              budget_kaggle            Keep Kaggle; fill in zeros with Wikipedia data.\n",
    "# box_office               revenue                  Keep Kaggle; fill in zeros with Wikipedia data.\n",
    "# release_date_wiki        release_date_kaggle      Drop Wikipedia.\n",
    "# Language                 original_language        Drop Wikipedia.\n",
    "# Production company(s)    production_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c4b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production Companies\n",
    "\n",
    "# [Inspect] \n",
    "# Again, we'll start off just taking a look at a small number of samples.\n",
    "\n",
    "movies_df[['Production company(s)','production_companies']]\n",
    "\n",
    "# The Kaggle data is much more consistent, and it would be difficult,\n",
    "# if not impossible, to translate the Wikipedia data into the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57824d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# We'll drop the Wikipedia data in this case.\n",
    "\n",
    "# Competing data:\n",
    "# Wiki                     Movielens                Resolution\n",
    "#--------------------------------------------------------------------------\n",
    "# title_wiki               title_kaggle             Drop Wikipedia.\n",
    "# running_time             runtime                  Keep Kaggle; fill in zeros with Wikipedia data.\n",
    "# budget_wiki              budget_kaggle            Keep Kaggle; fill in zeros with Wikipedia data.\n",
    "# box_office               revenue                  Keep Kaggle; fill in zeros with Wikipedia data.\n",
    "# release_date_wiki        release_date_kaggle      Drop Wikipedia.\n",
    "# Language                 original_language        Drop Wikipedia.\n",
    "# Production company(s)    production_companies     Drop Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e52c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put It All Together\n",
    "\n",
    "# [Execute]\n",
    "\n",
    "# First, we'll drop the title_wiki, release_date_wiki, Language, and Production company(s) columns.\n",
    "movies_df.drop(columns=['title_wiki','release_date_wiki','Language','Production company(s)'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee0c3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, to save a little time,\n",
    "# we'll make a function that fills in missing data for a column pair and then drops the redundant column.\n",
    "\n",
    "def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "    df[kaggle_column] = df.apply(\n",
    "        lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column]\n",
    "        , axis=1)\n",
    "    df.drop(columns=wiki_column, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a99da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can run the function for the three column pairs that we decided to fill in zeros.\n",
    "\n",
    "fill_missing_kaggle_data(movies_df, 'runtime', 'running_time')\n",
    "fill_missing_kaggle_data(movies_df, 'budget_kaggle', 'budget_wiki')\n",
    "fill_missing_kaggle_data(movies_df, 'revenue', 'box_office')\n",
    "movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec1fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we've merged our data and filled in values,\n",
    "# it's good to check that there aren't any columns with only one value,\n",
    "# since that doesn't really provide any information.\n",
    "# Don't forget, we need to convert lists to tuples for value_counts() to work.\n",
    "\n",
    "for col in movies_df.columns:\n",
    "    lists_to_tuples = lambda x: tuple(x) if type(x) == list else x\n",
    "    value_counts = movies_df[col].apply(lists_to_tuples).value_counts(dropna=False)\n",
    "    num_values = len(value_counts)\n",
    "    if num_values == 1:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89833326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this, we see that 'video' only has one value:\n",
    "movies_df['video'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c1b753",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df['video']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# We should reorder the columns to make the dataset easier to read for the hackathon participants.\n",
    "# Having similar columns near each other helps people looking through the data\n",
    "# get a better sense of what information is available.\n",
    "\n",
    "# One way to reorder them would be to consider the columns roughly in groups, like this:\n",
    "\n",
    "# 1) Identifying information (IDs, titles, URLs, etc.)\n",
    "# 2) Quantitative facts (runtime, budget, revenue, etc.)\n",
    "# 3) Qualitative facts (genres, languages, country, etc.)\n",
    "# 4) Business data (production companies, distributors, etc.)\n",
    "# 5) People (producers, director, cast, writers, etc.)\n",
    "\n",
    "# The following code is one way to reorder the columns:\n",
    "\n",
    "movies_df = movies_df.loc[:, ['imdb_id','id','title_kaggle','original_title','tagline','belongs_to_collection','url','imdb_link',\n",
    "                       'runtime','budget_kaggle','revenue','release_date_kaggle','popularity','vote_average','vote_count',\n",
    "                       'genres','original_language','overview','spoken_languages','Country',\n",
    "                       'production_companies','production_countries','Distributor',\n",
    "                       'Producer(s)','Director','Starring','Cinematography','Editor(s)','Writer(s)','Composer(s)','Based on'\n",
    "                      ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12080838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Execute]\n",
    "\n",
    "# Finally, we need to rename the columns to be consistent.\n",
    "\n",
    "movies_df.rename({'id':'kaggle_id',\n",
    "                  'title_kaggle':'title',\n",
    "                  'url':'wikipedia_url',\n",
    "                  'budget_kaggle':'budget',\n",
    "                  'release_date_kaggle':'release_date',\n",
    "                  'Country':'country',\n",
    "                  'Distributor':'distributor',\n",
    "                  'Producer(s)':'producers',\n",
    "                  'Director':'director',\n",
    "                  'Starring':'starring',\n",
    "                  'Cinematography':'cinematography',\n",
    "                  'Editor(s)':'editors',\n",
    "                  'Writer(s)':'writers',\n",
    "                  'Composer(s)':'composers',\n",
    "                  'Based on':'based_on'\n",
    "                 }, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1324a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE\n",
    "# If you did not use .loc to reorder the columns and instead passed a list of column names\n",
    "# to the indexing operator (i.e. movies_df = movies_df[[‘imdb_id’, ‘title_kaggle’, … ]]),\n",
    "# you may receive a SettingWithCopyWarning. Don't panic! This isn't an error, so your code will continue to work,\n",
    "# but it is a warning that your code may not behave as you expect.\n",
    "# In this case, your code will work fine, but for best practices, use .loc instead to avoid this warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORM AND MERGE RATING DATA\n",
    "\n",
    "# For each movie, Britta wants to include the rating data, but the rating dataset has so much information\n",
    "# that it's too unwieldy to use all of it. We could calculate some basic statistics like the mean and median\n",
    "# rating for each movie, but a more useful summary is just to count how many times a movie received a given rating.\n",
    "# This way, someone who wants to calculate statistics for the dataset would have all the information they need. \n",
    "\n",
    "# We'll include the raw ratings data if the hackathon participants want to do more in-depth analysis,\n",
    "# such as comparing across users, but having the rating counts for each movie is easy enough to do.\n",
    "# Plus, it will enable the hackathon participants to calculate statistics on their own without having to work\n",
    "# with a dataset containing 26-million rows.\n",
    "\n",
    "# First, we need to use a groupby on the \"movieId\" and \"rating\" columns and take the count for each group.\n",
    "rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bcd255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we'll rename the \"userId\" column to \"count.\"\n",
    "\n",
    "# The choice of renaming \"userId\" to \"count\" is arbitrary.\n",
    "# Both \"userId\" and \"timestamp\" have the same information, so we could use either one.\n",
    "# Your code should look like the following:\n",
    "\n",
    "rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "                .rename({'userId':'count'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45549d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Execute]\n",
    "\n",
    "# We can pivot this data so that movieId is the index,the columns will be all the rating values,\n",
    "# and the rows will be the counts for each rating value.\n",
    "\n",
    "rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "                .rename({'userId':'count'}, axis=1) \\\n",
    "                .pivot(index='movieId',columns='rating', values='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63257680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to rename the columns so they're easier to understand.\n",
    "# We'll prepend rating_ to each column with a list comprehension:\n",
    "\n",
    "rating_counts.columns = ['rating_' + str(col) for col in rating_counts.columns]\n",
    "\n",
    "# Now we can merge the rating counts into movies_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676dd7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time, we need to use a left merge, since we want to keep everything in movies_df:\n",
    "movies_with_ratings_df = pd.merge(movies_df, rating_counts, left_on='kaggle_id', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6426e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Plan]\n",
    "\n",
    "# Finally, because not every movie got a rating for each rating level, there will be missing values instead of zeros.\n",
    "# We have to fill those in ourselves, like this:\n",
    "\n",
    "movies_with_ratings_df[rating_counts.columns] = movies_with_ratings_df[rating_counts.columns].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdd0ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.5.1 Connect Pandas and SQL\n",
    "\n",
    "# For our local server, the connection string will be as follows:\n",
    "db_string = f\"postgresql://postgres:{db_password}@127.0.0.1:5432/movie_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c25467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the database engine with the following line in a new cell:\n",
    "engine = create_engine(db_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01286ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Movie Data\n",
    "# To save the movies_df DataFrame to a SQL table, we only have to specify the name of the table\n",
    "# and the engine in the to_sql() method.\n",
    "\n",
    "movies_df.to_sql(name='movies', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9671f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Ratings Data\n",
    "# The ratings data is too large to import in one statement, so it has to be divided into \"chunks\" of data.\n",
    "# To do so, we'll need to reimport the CSV using the chunksize= parameter in read_csv().\n",
    "# This creates an iterable object,\n",
    "# so we can make a for loop and append the chunks of data to the new rows to the target SQL table.\n",
    "\n",
    "# CAUTION\n",
    "# The to_sql() method also has a chunksize= parameter, but that won't help us with memory concerns.\n",
    "# The chunksize= parameter in to_sql() creates smaller transactions sent to SQL to prevent the SQL \n",
    "# instance from getting locked up with a large transaction.\n",
    "\n",
    "# The simplest way to do this is with two lines:\n",
    "\n",
    "# Do not run this yet!\n",
    "# for data in pd.read_csv(f'{file_dir}/ratings.csv', chunksize=1000000):\n",
    "    # data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "    \n",
    "# This can take quite a long time to run (more than an hour).\n",
    "# It's a really good idea to print out some information about how it's running.\n",
    "# Let's add functionality to this code to print out:\n",
    "\n",
    "# How many rows have been imported\n",
    "# How much time has elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63932cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Print Number of Imported Rows\n",
    "# Below is the previous block of code, with comments added for refactoring:\n",
    "\n",
    "# create a variable for the number of rows imported\n",
    "# for data in pd.read_csv(f'{file_dir}/ratings.csv', chunksize=1000000):\n",
    "\n",
    "    # print out the range of rows that are being imported\n",
    "\n",
    "    # data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "\n",
    "    # increment the number of rows imported by the chunksize\n",
    "\n",
    "    # print that the rows have finished importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9249425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable for the number of rows imported\n",
    "# We'll call the new variable rows_imported and give it the value 0 to start.\n",
    "\n",
    "# rows_imported = 0\n",
    "# for data in pd.read_csv(f'{file_dir}/ratings.csv', chunksize=1000000):\n",
    "\n",
    "    # print out the range of rows that are being imported\n",
    "\n",
    "    # data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "\n",
    "    # increment the number of rows imported by the size of 'data'\n",
    "\n",
    "    # print that the rows have finished importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the range of rows that are being imported\n",
    "# When printing out monitoring information, it's generally a good practice to print out when a process is beginning\n",
    "# and when a process has ended successfully, because if there's any problem,\n",
    "# we have a better sense of which process caused the problem by seeing what part never finished successfully.\n",
    "\n",
    "# On top of this, it's good practice to keep both outputs on the same line,\n",
    "# because it's easier to monitor which step is currently being performed.\n",
    "# To do this,we use the end= parameter in the print function.\n",
    "# Setting the end to an empty string will prevent the output from going to the next line.\n",
    "\n",
    "# create a variable for the number of rows imported\n",
    "# rows_imported = 0\n",
    "# for data in pd.read_csv(f'{file_dir}/ratings.csv', chunksize=1000000):\n",
    "\n",
    "    # print out the range of rows that are being imported\n",
    "    # print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "\n",
    "    # data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "\n",
    "    # increment the number of rows imported by the size of 'data'\n",
    "\n",
    "    # print that the rows have finished importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652f708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increment the number of rows imported by the size of 'data'\n",
    "# This is a great time to use the compound operator += to add the length of the data read in to rows_imported.\n",
    "\n",
    "# REWIND\n",
    "# Remember, compound operators are shortcuts to perform a simple arithmetic operation on a variable and\n",
    "# reassign the new value to the variable.\n",
    "\n",
    "# For example, foo += 1 is equivalent to foo = foo + 1.\n",
    "\n",
    "# create a variable for the number of rows imported\n",
    "# rows_imported = 0\n",
    "# for data in pd.read_csv(f'{file_dir}/ratings.csv', chunksize=1000000):\n",
    "\n",
    "    # print out the range of rows that are being imported\n",
    "    # print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "\n",
    "    # data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "\n",
    "    # increment the number of rows imported by the size of 'data'\n",
    "    # rows_imported += len(data)\n",
    "\n",
    "    # print that the rows have finished importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6afdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print that the rows have finished importing\n",
    "# Finally, we can print that everything was imported successfully. We don't need to specify an end= parameter in the printfunction since we do want a new line printed now.\n",
    "\n",
    "# create a variable for the number of rows imported\n",
    "# rows_imported = 0\n",
    "# for data in pd.read_csv(f'{file_dir}/ratings.csv', chunksize=1000000):\n",
    "\n",
    "    # print out the range of rows that are being imported\n",
    "    # print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "\n",
    "    # data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "\n",
    "    # increment the number of rows imported by the size of 'data'\n",
    "    # rows_imported += len(data)\n",
    "\n",
    "    # print that the rows have finished importing\n",
    "    # print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2d795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we're done refactoring, we can delete our comments.\n",
    "\n",
    "# rows_imported = 0\n",
    "# for data in pd.read_csv(f'{file_dir}/ratings.csv', chunksize=1000000):\n",
    "\n",
    "    # print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "    # data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "    # rows_imported += len(data)\n",
    "\n",
    "    # print(f'Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6035f458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an optional step, but it's a good idea when running a long process.\n",
    "# We're going to print the total amount of time elapsed at every step.\n",
    "# This is useful to estimate how long the process is going to take.\n",
    "\n",
    "# We'll use the built-in time module in Python. time.time() returns the current time whenever it is called.\n",
    "# Subtracting two time values gives the difference in seconds. By setting a variable at the beginning to the time at the start,\n",
    "# inside the loop we can easily calculate elapsed time and print it out.\n",
    "\n",
    "# First, we'll add the following to our import cell and rerun it:\n",
    "\n",
    "# import time\n",
    "# Add two new comments: one before the for loop, and one inside the for loop,\n",
    "# right before the last final print() statement. The first comment is to get the start time from time.time(),\n",
    "# and the second comment is to add the elapsed time to the final printout.\n",
    "\n",
    "\n",
    "\n",
    "# rows_imported = 0\n",
    "# get the start_time from time.time()\n",
    "# for data in pd.read_csv(f'{file_dir}/ratings.csv', chunksize=1000000):\n",
    "\n",
    "    # print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "    # data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "    # rows_imported += len(data)\n",
    "\n",
    "    # add elapsed time to final print out\n",
    "    # print(f'Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a259f83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the start_time from time.time()\n",
    "# The start_time = time.time() method will initialize the start_time with the current time.\n",
    "\n",
    "# rows_imported = 0\n",
    "# get the start_time from time.time()\n",
    "# start_time = time.time()\n",
    "# for data in pd.read_csv(f'{file_dir}/ratings.csv', chunksize=1000000):\n",
    "    # print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "    # data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "    # rows_imported += len(data)\n",
    "\n",
    "    # add elapsed time to final print out\n",
    "    # print(f'Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82aae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The elapsed time is simply time.time() - start_time, which can be added directly into the f-string.\n",
    "\n",
    "rows_imported = 0\n",
    "# get the start_time from time.time()\n",
    "start_time = time.time()\n",
    "for data in pd.read_csv(f'{file_dir}/ratings.csv', chunksize=1000000):\n",
    "    print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "    data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "    rows_imported += len(data)\n",
    "\n",
    "    # add elapsed time to final print out\n",
    "    print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
